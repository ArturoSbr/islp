Chapter 2 - Statistical Learning

---

1. For each of parts (a) through (d), indicate whether we would generally expect
the performance of a flexible statistical learning method to be better or worse
than an inflexible method. Justify your answer.

    a) Given that the stochastic component of $y = f(x) + \epsilon$ has mean
    zero, a large number of samples will closely average out to zero for any
    given value of $x$. Thus, overfitting to the training data's noise should
    not be a problem for a flexible method.

    b) An inflexible method should perform better because a flexible model will
    likely overfit due to the small sample size and the high dimensionality of
    the training data.

    c) A more flexible method should outperform a parsimonious model because
    inflexible methods cannot typically capture non-linear patterns in the
    training data.

    d) Since we have no information regarding sample size, we expect an
    inflexible method to perform better. Due to the high variance of the error
    terms, a more flexible approach is likely to learn false patterns from the
    errors.

2. Explain whether each scenario is a classification or regression problem, and
indicate whether we are most interested in inference or pre- diction. Finally,
provide $n$ and $p$.

    a) Regression, inference. $n = 500$, $p = 3$.
    
    b) Classification, prediction, $n = 20$, $p = 13$.

    c) Regression, predictions, $n = 52$, $p = 3$.

3. We now revisit the bias-variance decomposition.

    a) ![figure-ch02-3a](
        https://drive.google.com/uc?id=1zXTYjQ7-B0cuT2XvqPqlycKo24cGXPK4
    )

    b) As flexibility increases, squared bias tends to zero because a set of
    hyperparameters that allow for enoughÂ flexibility will enable the model to
    adjust to the true form of $f(x)$. Variance starts off low because the fit
    of inflexible methods do not vary much from one dataset to another. However,
    for high levels of flexibility, the fit of the model will vary greatly with
    even the slightest of changes in the training dataset. The training error
    starts off high and decreases monotonically because the model can fit to the
    training data increasingly better as its complexity increases. The testing
    error starts off high because the model is unlikely to be a good
    approximation for $f(x)$ when  flexibility is low. Likewise, when
    flexibility is too high, the model will overfit and perform poorly on
    testing data. For levels of complexity somewhere in the middle, the model
    will be a good approximation of $f(x)$ (low bias) and will not be
    overfitting to the training data (low variance). Hence, the training error
    is low for these levels of complexity. Finally, the irreducible error is a
    constant ( $V(\epsilon)$ ).

4. You will now think of some real-life applications for statistical learning
(we only present one example per question).

    a) To predict if an applicant will default on a loan. $y \in \{0, 1\}$,
    where $y = 1$ implies the applicant defaulted on the loan and $y = 0$ means
    they paid off the loan. The predictors are the applicant's age, their
    debt-to-income ratio and their monthly income measured in thousands of US
    dollars.

    b) To estimate the causal effect of a drug on a patient's weight. The
    independent variables are a treatment indicator (a binary variable that
    takes on the value of $1$ if the patient received the drug and $0$ if they
    received a placebo), age, sex and the time the patient spends exercising
    (measured in hours per week).

    c) To find segments of viewers in Netflix's subscribers. There is no
    response and the features are age, sex, daily watch time and subscription
    tier.

5. What are the advantages and disadvantages of a very flexible (versus a less
flexible) approach for regression or classification? Under what circumstances
might a more flexible approach be preferred to a less flexible approach? When
might a less flexible approach be preferred?

    The advantages of a flexible approach include being able to model highly
    non-linear relationships and reduce the training error to zero. However,
    they may overfit to the training data, they require more data points to
    generalize well, the reasons behind their predictions may be hard or
    impossible to interpret and they require more processing power to fit.

    A flexible approach is preferred when we have a large dataset, are mainly
    interested in generating accurate predictions and are not constrained by
    computing resources. An inflexible approach is preferred when we are
    interested in interpreting the model, have few
    training observations or know that the variance of the error term is large.

6. Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric
approach to regression or classification (as opposed to a non-parametric
approach)? What are its disadvantages?

    A parametric approach makes assumptions about the form of the underlying
    data-generating process ( $f(x)$ ). For instance, linear regression assumes
    that $f(x)$ is linear and additive combination of the predictors. A
    non-parametric approach makes no such assumptions and therefore has more
    parameters to estimate. Parametric approaches are usually interpretable and
    fit faster than non-parametric models. On the other hand, by not making any
    assumptions about the form of $f(x)$, non-parametric approaches may result
    in more accurate estimates.

7. The table below provides a training data set containing six observations,
three predictors, and one qualitative response variable. Suppose we wish to use
this data set to make a prediction for Y when $X_1 = X_2 = X_3 = 0$ using
_K_-nearest neighbors.

    a)

    $d(x_0, obs_1) = \sqrt{9}$

    $d(x_0, obs_2) = \sqrt{4}$

    $d(x_0, obs_3) = \sqrt{10}$

    $d(x_0, obs_4) = \sqrt{5}$

    $d(x_0, obs_5) = \sqrt{2}$

    $d(x_0, obs_6) = \sqrt{3}$

    b) $\hat{f}(x_0; K=1) = \text{Green}$ because $obs_2$ is its closest
    neighbor.

    c) $\hat{f}(x_0; K=3) = \text{Red}$ because $\text{Red}$ is the most common
    occurrence in the three-neighbor neighborhood of $x_0$.

    d) A small value for $K$ implies more flexibility. Hence, a small value for
    $K$ should perform better when the Bayes decision boundary is highly 
    non-linear.