Chapter 3 - Linear Regression

---

1. Describe the null hypotheses to which the $p$-values given in Table 3.4
correspond. Explain what conclusions you can draw based on these $p$-values.
Your explanation should be phrased in terms of sales, TV, radio, and newspaper,
rather than in terms of the coefficients of the linear model.

2. Carefully explain the differences between the KNN classifier and KNN
regression methods.

3. Suppose we have a data set with five predictors, $X_1 =$ GPA, $X_2 =$ IQ,
$X_3 =$ Level ($1$ for College and $0$ for High School), $X_4 =$ Interaction between GPA and IQ, and $X_5 =$ Interaction between GPA and Level. The response
is starting salary after graduation (in thousands of dollars). Suppose we use
least squares to fit the model, and get
$\hat{\beta_0} = 50$, $\hat{\beta_1} = 20$, $\hat{\beta_2} = 0.07$,
$\hat{\beta_3} = 35$, $\hat{\beta_4} = 0.01$, $\hat{\beta_0} = âˆ’10$.


4. I collect a set of data ($n = 100$ observations) containing a single
predictor and a quantitative response. I then fit a linear regression model to
the data, as well as a separate cubic regression, i.e.
$Y = \hat{\beta_0} + \hat{\beta_1} X^1 + \hat{\beta_2} X^2 +
\hat{\beta_3} X^3 + \epsilon$.


5. Consider the fitted values that result from performing linear regression
without an intercept.

6. Using (3.4), argue that in the case of simple linear regression, the least
squares line always passes through the point $(\bar{x}, \bar{y})$.

7. It is claimed in the text that in the case of simple linear regression of $Y$
onto $X$, the $R^2$ statistic (3.17) is equal to the square of the correlation between $X$ and $Y$ (3.18). Prove that this is the case. For simplicity, you may assume that $\bar{x} = \bar{y} = 0$.